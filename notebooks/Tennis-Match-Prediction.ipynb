{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df89b9d",
   "metadata": {},
   "source": [
    "# Tennis Match Prediction with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43183b8e",
   "metadata": {},
   "source": [
    "## Match parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded156cd",
   "metadata": {},
   "source": [
    "‚úÖ Donn√©es d‚Äôentr√©e n√©cessaires\n",
    "Voici les caract√©ristiques essentielles (features) √† fournir pour chaque match :\n",
    "\n",
    "|Variable|\tType|\tDescription|\n",
    "| --- | --- | --- |\n",
    "|Rank_1|\tint|\tClassement ATP de Player_1|\n",
    "|Rank_2|\tint|\tClassement ATP de Player_2|\n",
    "|Pts_1|\tint|\tPoints ATP de Player_1|\n",
    "|Pts_2|\tint|\tPoints ATP de Player_2|\n",
    "|Odd_1|\tfloat|\tCote pr√©-match de Player_1 (optionnelle mais utile)|\n",
    "|Odd_2|\tfloat|\tCote pr√©-match de Player_2|\n",
    "|Surface|\tstr|\tSurface du match (Hard, Clay, Grass)|\n",
    "|Round|\tstr|\tTour du match (1st Round, Quarterfinal, etc.)|\n",
    "|Best of|\tint|\tNombre de sets gagnants (3 ou 5)|\n",
    "|Court|\tstr|\tIndoor / Outdoor|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c0101",
   "metadata": {},
   "source": [
    "## Install librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86774a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232b818",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"Essaie plusieurs formats de date.\"\"\"\n",
    "    for fmt in ('%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%d/%m/%Y %H:%M', '%d/%m/%Y'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def filtrer_et_normaliser_matchs(fichier_entree, fichier_sortie):\n",
    "    maintenant = datetime.now()\n",
    "    trois_mois = timedelta(days=1825)\n",
    "    matchs_conserves = 0\n",
    "\n",
    "    with open(fichier_entree, mode='r', encoding='utf-8') as infile, \\\n",
    "         open(fichier_sortie, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        lecteur = csv.DictReader(infile, delimiter=',')\n",
    "        champs = lecteur.fieldnames\n",
    "        ecrivain = csv.DictWriter(outfile, fieldnames=champs, delimiter=',')\n",
    "        ecrivain.writeheader()\n",
    "\n",
    "        for ligne in lecteur:\n",
    "            date_str = ligne.get('Date', '')\n",
    "            date_match = parse_date_flexible(date_str)\n",
    "            if date_match and (maintenant - date_match <= trois_mois):\n",
    "                # Normalisation de la date\n",
    "                ligne['Date'] = date_match.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                ecrivain.writerow(ligne)\n",
    "                matchs_conserves += 1\n",
    "\n",
    "    print(f\"{matchs_conserves} matchs conserv√©s (jou√©s dans les 5 dernieres ann√©es).\")\n",
    "\n",
    "\n",
    "# Extraction des matchs ATP des 3 derniers mois\n",
    "filtrer_et_normaliser_matchs('../data/atp_tennis.csv', '../data/matches_atp_5_dernieres_ann√©es.csv')\n",
    "\n",
    "# Extraction des matchs WTA des 3 derniers mois\n",
    "#filtrer_et_normaliser_matchs('../data/wta.csv', '../data/matches_wta_3_derniers_mois.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8af2d",
   "metadata": {},
   "source": [
    "#### Constitution du DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "### 1. Chargement des donn√©es\n",
    "df = pd.read_csv(\"../data/matches_atp_5_dernieres_ann√©es.csv\")\n",
    "\n",
    "def count_total_games(score_str):\n",
    "    if pd.isna(score_str):\n",
    "        return None\n",
    "    sets = re.findall(r'(\\d+)-(\\d+)', score_str)\n",
    "    return sum(int(a) + int(b) for a, b in sets)\n",
    "\n",
    "df['Total_Games'] = df['Score'].apply(count_total_games)\n",
    "df = df.dropna(subset=['Total_Games'])\n",
    "\n",
    "# üéØ Classes de total de jeux\n",
    "bins = list(range(15, 40, 4))\n",
    "labels = [f\"{b}-{b+3}\" for b in bins[:-1]]\n",
    "df[\"Games_Class\"] = pd.cut(df[\"Total_Games\"], bins=bins, labels=labels, include_lowest=True)\n",
    "df = df.dropna(subset=[\"Games_Class\"])\n",
    "\n",
    "# üéØ Features de base\n",
    "df[\"Rank_Diff\"] = df[\"Rank_1\"] - df[\"Rank_2\"]\n",
    "df[\"Pts_Diff\"] = df[\"Pts_1\"] - df[\"Pts_2\"]\n",
    "df[\"Odds_Ratio\"] = df[\"Odd_1\"] / df[\"Odd_2\"]\n",
    "df[\"Book_Fav\"] = (df[\"Odd_1\"] < df[\"Odd_2\"]).astype(int)\n",
    "df[\"Avg_Rank\"] = (df[\"Rank_1\"] + df[\"Rank_2\"]) / 2\n",
    "df[\"Odd_Diff\"] = abs(df[\"Odd_1\"] - df[\"Odd_2\"])\n",
    "\n",
    "round_order = {\n",
    "    \"1st Round\": 1, \"2nd Round\": 2, \"3rd Round\": 3, \"4th Round\": 4,\n",
    "    \"Quarterfinal\": 5, \"Semifinal\": 6, \"Final\": 7\n",
    "}\n",
    "df[\"Round_Ordinal\"] = df[\"Round\"].map(round_order)\n",
    "\n",
    "### 2. Nettoyage\n",
    "df = df.dropna(subset=['Rank_1', 'Rank_2', 'Pts_1', 'Pts_2', 'Odd_1', 'Odd_2', 'Surface', 'Round', 'Best of', 'Court'])\n",
    "df['Winner_encoded'] = (df['Winner'] == df['Player_1']).astype(int)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "### 3. Chargement historique complet pour H2H + Forme\n",
    "full_df = pd.read_csv(\"../data/atp_tennis.csv\")\n",
    "full_df = full_df.dropna(subset=[\"Player_1\", \"Player_2\", \"Winner\", \"Date\"])\n",
    "full_df[\"Date\"] = pd.to_datetime(full_df[\"Date\"], errors='coerce')\n",
    "full_df = full_df.dropna(subset=[\"Date\"])\n",
    "full_df = full_df.sort_values(\"Date\")\n",
    "\n",
    "### 4. Historique H2H\n",
    "h2h_dict = defaultdict(lambda: [0, 0])\n",
    "for _, row in full_df.iterrows():\n",
    "    p1, p2, winner = row[\"Player_1\"], row[\"Player_2\"], row[\"Winner\"]\n",
    "    key = tuple(sorted([p1, p2]))\n",
    "    if winner == p1:\n",
    "        h2h_dict[key][0] += 1\n",
    "    elif winner == p2:\n",
    "        h2h_dict[key][1] += 1\n",
    "\n",
    "def get_h2h(p1, p2):\n",
    "    key = tuple(sorted([p1, p2]))\n",
    "    h2h = h2h_dict.get(key, [0, 0])\n",
    "    return h2h if p1 <= p2 else h2h[::-1]\n",
    "\n",
    "df[[\"H2H_P1\", \"H2H_P2\"]] = df.apply(lambda row: pd.Series(get_h2h(row[\"Player_1\"], row[\"Player_2\"])), axis=1)\n",
    "df[\"H2H_Diff\"] = df[\"H2H_P1\"] - df[\"H2H_P2\"]\n",
    "\n",
    "### 5. Forme r√©cente (5 derniers matchs)\n",
    "recent_form = defaultdict(lambda: deque(maxlen=5))\n",
    "win_history = {}\n",
    "\n",
    "for i, row in full_df.iterrows():\n",
    "    p1, p2, winner = row[\"Player_1\"], row[\"Player_2\"], row[\"Winner\"]\n",
    "    win_p1 = sum(recent_form[p1])\n",
    "    win_p2 = sum(recent_form[p2])\n",
    "    win_history[i] = (win_p1, win_p2)\n",
    "    recent_form[p1].append(1 if winner == p1 else 0)\n",
    "    recent_form[p2].append(1 if winner == p2 else 0)\n",
    "\n",
    "recent_form_dict = {player: sum(matches) for player, matches in recent_form.items()}\n",
    "\n",
    "# Cr√©er DataFrame temporaire\n",
    "win_df = pd.DataFrame.from_dict(win_history, orient='index', columns=[\"Wins_Last5_P1\", \"Wins_Last5_P2\"])\n",
    "full_df = full_df.reset_index(drop=True)\n",
    "full_df = pd.concat([full_df, win_df], axis=1)\n",
    "\n",
    "# Fusion avec df\n",
    "df = pd.merge(df, full_df[[\"Player_1\", \"Player_2\", \"Date\", \"Wins_Last5_P1\", \"Wins_Last5_P2\"]],\n",
    "              on=[\"Player_1\", \"Player_2\", \"Date\"], how=\"left\")\n",
    "\n",
    "df[\"Form_Diff\"] = df[\"Wins_Last5_P1\"] - df[\"Wins_Last5_P2\"]\n",
    "df[[\"Wins_Last5_P1\", \"Wins_Last5_P2\", \"Form_Diff\"]] = df[[\"Wins_Last5_P1\", \"Wins_Last5_P2\", \"Form_Diff\"]].fillna(0)\n",
    "\n",
    "### 6. D√©finition des features finales\n",
    "numeric_features = [\n",
    "    'Rank_1', 'Rank_2', 'Pts_1', 'Pts_2', 'Odd_1', 'Odd_2', 'Best of',\n",
    "    'Rank_Diff', 'Pts_Diff', 'Odds_Ratio', 'Book_Fav', 'Avg_Rank',\n",
    "    'Odd_Diff', 'Round_Ordinal',\n",
    "    'H2H_P1', 'H2H_P2', 'H2H_Diff',\n",
    "    'Wins_Last5_P1', 'Wins_Last5_P2', 'Form_Diff'\n",
    "]\n",
    "\n",
    "categorical_features = ['Surface', 'Court']\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "# 4. Sauvegarde des dictionnaires\n",
    "joblib.dump(dict(h2h_dict), \"../models/h2h_dict.pkl\")\n",
    "joblib.dump(recent_form_dict, \"../models/recent_form_dict.pkl\")\n",
    "\n",
    "# Final dataset\n",
    "#X = df[all_features]\n",
    "#y = df[\"Winner_encoded\"]\n",
    "\n",
    "\n",
    "# V√©rification\n",
    "print(\"‚úÖ Dataset pr√™t. Nombre de lignes :\", len(df))\n",
    "print(\"‚úÖ Features num√©riques :\", numeric_features)\n",
    "print(\"‚úÖ X shape :\", X.shape, \"| y shape :\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# üîß Cr√©ation des features pour le mod√®le Bradley-Terry-like\n",
    "df_bt = pd.DataFrame()\n",
    "\n",
    "# Diff√©rences entre Player_1 et Player_2\n",
    "for col in numeric_features:\n",
    "    if \"_1\" in col and col.replace(\"_1\", \"_2\") in df.columns:\n",
    "        col_2 = col.replace(\"_1\", \"_2\")\n",
    "        diff_col = col.replace(\"_1\", \"_Diff\")\n",
    "        df_bt[diff_col] = df[col] - df[col_2]\n",
    "\n",
    "# Ajout des features diff√©rentielles manuelles\n",
    "df_bt[\"Form_Diff\"] = df[\"Form_Diff\"]\n",
    "df_bt[\"H2H_Diff\"] = df[\"H2H_Diff\"]\n",
    "\n",
    "# Ajouter les features cat√©gorielles\n",
    "df_bt[categorical_features] = df[categorical_features]\n",
    "\n",
    "# Cible\n",
    "y_bt = df[\"Winner_encoded\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f5d77",
   "metadata": {},
   "source": [
    "### Boucle rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78272599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, brier_score_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# üßπ Mise √† jour du preprocessing pour int√©grer StandardScaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())  # ‚úÖ Ajout du scaler ici\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# üîß Param√®tres de la rolling window\n",
    "train_window_months = 6\n",
    "start_date = pd.to_datetime(\"2019-01-01\")\n",
    "end_date = pd.to_datetime(\"2024-12-31\")\n",
    "\n",
    "df = df.sort_values(\"Date\").dropna(subset=[\"Date\"])\n",
    "\n",
    "rolling_results = []\n",
    "current_date = start_date\n",
    "\n",
    "while current_date + pd.DateOffset(months=train_window_months + 1) <= end_date:\n",
    "    train_end = current_date + pd.DateOffset(months=train_window_months)\n",
    "    test_start = train_end\n",
    "    test_end = test_start + pd.DateOffset(months=1)\n",
    "\n",
    "    df_train = df[(df[\"Date\"] >= current_date) & (df[\"Date\"] < train_end)]\n",
    "    df_test = df[(df[\"Date\"] >= test_start) & (df[\"Date\"] < test_end)]\n",
    "\n",
    "    if len(df_train) < 200 or len(df_test) < 50:\n",
    "        current_date += pd.DateOffset(months=1)\n",
    "        continue\n",
    "\n",
    "    X_train = df_train[all_features]\n",
    "    y_train = df_train[\"Winner_encoded\"]\n",
    "    X_test = df_test[all_features]\n",
    "    y_test = df_test[\"Winner_encoded\"]\n",
    "\n",
    "    # ‚úÖ Logistic Regression avec solver robuste\n",
    "    base_model = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, solver='saga', penalty='l2', random_state=42))\n",
    "    ])\n",
    "    base_model.fit(X_train, y_train)\n",
    "\n",
    "    try:\n",
    "        cal_iso = CalibratedClassifierCV(estimator=base_model, method='isotonic', cv=3)\n",
    "        cal_iso.fit(X_train, y_train)\n",
    "        proba_iso = cal_iso.predict_proba(X_test)[:, 1]\n",
    "        brier_iso = brier_score_loss(y_test, proba_iso)\n",
    "\n",
    "        cal_sig = CalibratedClassifierCV(estimator=base_model, method='sigmoid', cv=3)\n",
    "        cal_sig.fit(X_train, y_train)\n",
    "        proba_sig = cal_sig.predict_proba(X_test)[:, 1]\n",
    "        brier_sig = brier_score_loss(y_test, proba_sig)\n",
    "\n",
    "        if brier_iso < brier_sig:\n",
    "            y_proba = proba_iso\n",
    "            selected_cal = \"isotonic\"\n",
    "        else:\n",
    "            y_proba = proba_sig\n",
    "            selected_cal = \"sigmoid\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Calibration √©chou√©e pour {test_start.strftime('%Y-%m')} : {e}\")\n",
    "        y_proba = base_model.predict_proba(X_test)[:, 1]\n",
    "        selected_cal = \"non_calibr√©\"\n",
    "\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    rolling_results.append({\n",
    "        \"Train_start\": current_date.strftime(\"%Y-%m\"),\n",
    "        \"Train_end\": train_end.strftime(\"%Y-%m\"),\n",
    "        \"Test_month\": test_start.strftime(\"%Y-%m\"),\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"LogLoss\": log_loss(y_test, y_proba),\n",
    "        \"Brier\": brier_score_loss(y_test, y_proba),\n",
    "        \"Calibration\": selected_cal\n",
    "    })\n",
    "\n",
    "    print(f\"üìÖ {test_start.strftime('%Y-%m')} | Cal: {selected_cal} | Acc: {rolling_results[-1]['Accuracy']:.3f} | AUC: {rolling_results[-1]['AUC']:.3f}\")\n",
    "\n",
    "    current_date += pd.DateOffset(months=1)\n",
    "\n",
    "# üìä R√©sum√©\n",
    "rolling_df = pd.DataFrame(rolling_results)\n",
    "display(rolling_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cd3b0",
   "metadata": {},
   "source": [
    "#### Custom Data Extraction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49c79b",
   "metadata": {},
   "source": [
    "#### Applying Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df292b1",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c27e9",
   "metadata": {},
   "source": [
    "### Import des librairies et chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f483d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, recall_score,\n",
    "    f1_score, log_loss, balanced_accuracy_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6fca6",
   "metadata": {},
   "source": [
    "### Entraine un mod√®le de r√©gression pour le vainqueur du match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split sp√©cifique pour le mod√®le BT\n",
    "X_train_bt, X_test_bt, y_train_bt, y_test_bt = train_test_split(df_bt, y_bt, stratify=y_bt, test_size=0.2, random_state=42)\n",
    "\n",
    "### 5. Preprocessing\n",
    "# Transformer pour les variables num√©riques\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Transformer pour les variables cat√©gorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Remplace les NaN par la modalit√© la plus fr√©quente\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combiner les transformations\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "\n",
    "### 6. Mod√®les\n",
    "models = {\n",
    "    \"BradleyTerry_LogReg\": (\n",
    "        Pipeline([\n",
    "            (\"preprocessor\", ColumnTransformer([\n",
    "                (\"num\", StandardScaler(), [\n",
    "                    col for col in df_bt.columns if col not in categorical_features\n",
    "                ]),\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "            ])),\n",
    "            (\"clf\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "        ]),\n",
    "        {\n",
    "            \"clf__C\": [0.1, 1.0, 10.0]  # Valeurs de r√©gularisation\n",
    "        }\n",
    "    ),\"DecisionTree\": (\n",
    "        Pipeline([(\"preprocessor\", preprocessor),\n",
    "                  (\"clf\", DecisionTreeClassifier(random_state=42))]),\n",
    "        {\"clf__max_depth\": [3, 5, 10], \"clf__min_samples_split\": [2, 5, 10]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        Pipeline([(\"preprocessor\", preprocessor),\n",
    "                  (\"clf\", RandomForestClassifier(random_state=42))]),\n",
    "        {\"clf__n_estimators\": [50, 150], \"clf__max_depth\": [5, 10, None]}\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        Pipeline([(\"preprocessor\", preprocessor),\n",
    "                  (\"clf\", GradientBoostingClassifier(random_state=42))]),\n",
    "        {\"clf__n_estimators\": [50, 150], \"clf__max_depth\": [3, 7], \"clf__learning_rate\": [0.05, 0.1]}\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        Pipeline([(\"preprocessor\", preprocessor),\n",
    "                (\"clf\", XGBClassifier(eval_metric='logloss'))]),  # <- ligne modifi√©e\n",
    "        {\"clf__n_estimators\": [100, 200], \"clf__max_depth\": [3, 5], \"clf__learning_rate\": [0.05, 0.1]}\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "### 7. Entra√Ænement + √©valuation\n",
    "results = []\n",
    "best_global_model = None\n",
    "best_global_score = 0\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"\\nüîç Training {name}...\")\n",
    "\n",
    "    # S√©lectionner X/y selon le mod√®le\n",
    "    if name == \"BradleyTerry_LogReg\":\n",
    "        X_train_model, X_test_model = X_train_bt, X_test_bt\n",
    "        y_train_model, y_test_model = y_train_bt, y_test_bt\n",
    "    else:\n",
    "        X_train_model, X_test_model = X_train, X_test\n",
    "        y_train_model, y_test_model = y_train, y_test\n",
    "\n",
    "    # Entra√Ænement + recherche d'hyperparam√®tres\n",
    "    grid = GridSearchCV(model, params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    grid.fit(X_train_model, y_train_model)\n",
    "\n",
    "    best_model_uncalibrated = grid.best_estimator_\n",
    "\n",
    "    # Calibration automatique\n",
    "    try:\n",
    "        cal_iso = CalibratedClassifierCV(best_model_uncalibrated, method='isotonic', cv=5)\n",
    "        cal_iso.fit(X_train_model, y_train_model)\n",
    "        proba_iso = cal_iso.predict_proba(X_test_model)[:, 1]\n",
    "        brier_iso = brier_score_loss(y_test_model, proba_iso)\n",
    "\n",
    "        cal_sig = CalibratedClassifierCV(best_model_uncalibrated, method='sigmoid', cv=5)\n",
    "        cal_sig.fit(X_train_model, y_train_model)\n",
    "        proba_sig = cal_sig.predict_proba(X_test_model)[:, 1]\n",
    "        brier_sig = brier_score_loss(y_test_model, proba_sig)\n",
    "\n",
    "        if brier_iso < brier_sig:\n",
    "            best_model = cal_iso\n",
    "            y_proba = proba_iso\n",
    "            selected_cal = \"isotonic\"\n",
    "        else:\n",
    "            best_model = cal_sig\n",
    "            y_proba = proba_sig\n",
    "            selected_cal = \"sigmoid\"\n",
    "\n",
    "        y_pred = best_model.predict(X_test_model)\n",
    "        brier_uncal = brier_score_loss(y_test_model, best_model_uncalibrated.predict_proba(X_test_model)[:, 1])\n",
    "\n",
    "        print(f\"üìê Calibration choisie pour {name} : {selected_cal} (Brier = {min(brier_iso, brier_sig):.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Calibration impossible pour {name} : {e}\")\n",
    "        best_model = best_model_uncalibrated\n",
    "        y_pred = best_model.predict(X_test_model)\n",
    "        y_proba = best_model.predict_proba(X_test_model)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "        selected_cal = \"non calibr√©\"\n",
    "        brier_iso = brier_sig = brier_uncal = None\n",
    "\n",
    "    # √âvaluation\n",
    "    acc = accuracy_score(y_test_model, y_pred)\n",
    "    prec = precision_score(y_test_model, y_pred)\n",
    "    rec = recall_score(y_test_model, y_pred)\n",
    "    f1 = f1_score(y_test_model, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test_model, y_pred)\n",
    "    auc = roc_auc_score(y_test_model, y_proba) if y_proba is not None else 0\n",
    "    logloss = log_loss(y_test_model, y_proba) if y_proba is not None else 0\n",
    "\n",
    "    results.append({\n",
    "        \"Mod√®le\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Balanced Accuracy\": bal_acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1,\n",
    "        \"ROC AUC\": auc,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Best Params\": grid.best_params_,\n",
    "        \"Calibration\": selected_cal,\n",
    "        \"Brier Avant\": brier_uncal,\n",
    "        \"Brier Apr√®s\": min(brier_iso, brier_sig) if selected_cal in [\"isotonic\", \"sigmoid\"] else brier_uncal\n",
    "    })\n",
    "\n",
    "    # Score global pond√©r√©\n",
    "    score_global = (\n",
    "        0.2 * acc +\n",
    "        0.2 * bal_acc +\n",
    "        0.3 * f1 +\n",
    "        0.2 * auc +\n",
    "        0.1 * prec\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Best Params: {grid.best_params_}\")\n",
    "    print(f\"üìä Accuracy: {acc:.3f} | üéØ F1: {f1:.3f} | üìâ ROC AUC: {auc:.3f} | üèÜ Score global: {score_global:.4f}\")\n",
    "\n",
    "    if score_global > best_global_score:\n",
    "        best_global_model = best_model\n",
    "        best_global_model_name = name\n",
    "        best_global_score = score_global\n",
    "\n",
    "    # 1. Calibrage isotonic\n",
    "    cal_iso = CalibratedClassifierCV(estimator=grid.best_estimator_, method='isotonic', cv=5)\n",
    "    cal_iso.fit(X_test, y_test)\n",
    "    proba_iso = cal_iso.predict_proba(X_test)[:, 1]\n",
    "    brier_iso = brier_score_loss(y_test, proba_iso)\n",
    "    prob_true_iso, prob_pred_iso = calibration_curve(y_test, proba_iso, n_bins=10)\n",
    "\n",
    "    # 2. Calibrage sigmoid (Platt scaling)\n",
    "    cal_sig = CalibratedClassifierCV(estimator=grid.best_estimator_, method='sigmoid', cv=5)\n",
    "    cal_sig.fit(X_test, y_test)\n",
    "    proba_sig = cal_sig.predict_proba(X_test)[:, 1]\n",
    "    brier_sig = brier_score_loss(y_test, proba_sig)\n",
    "    prob_true_sig, prob_pred_sig = calibration_curve(y_test, proba_sig, n_bins=10)\n",
    "\n",
    "    # 3. Calibration non calibr√©e (optionnel, pour comparaison)\n",
    "    if hasattr(grid.best_estimator_, \"predict_proba\"):\n",
    "        proba_uncal = grid.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "        brier_uncal = brier_score_loss(y_test, proba_uncal)\n",
    "        prob_true_uncal, prob_pred_uncal = calibration_curve(y_test, proba_uncal, n_bins=10)\n",
    "    else:\n",
    "        proba_uncal, brier_uncal = None, None\n",
    "\n",
    "    # 4. Affichage comparatif\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    if proba_uncal is not None:\n",
    "        plt.plot(prob_pred_uncal, prob_true_uncal, marker='o', label='Non calibr√©', color='gray')\n",
    "    plt.plot(prob_pred_iso, prob_true_iso, marker='o', label='Isotonic', color='green')\n",
    "    plt.plot(prob_pred_sig, prob_true_sig, marker='o', label='Sigmoid (Platt)', color='orange')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "    plt.title(\"üìê Courbes de calibration ‚Äì Comparaison\")\n",
    "    plt.xlabel(\"Probabilit√© pr√©dite\")\n",
    "    plt.ylabel(\"Proportion r√©elle de victoires\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Affichage Brier Scores\n",
    "    print(f\"Brier Score non calibr√© : {brier_uncal:.4f}\" if proba_uncal is not None else \"\")\n",
    "    print(f\"Brier Score isotonic    : {brier_iso:.4f}\")\n",
    "    print(f\"Brier Score sigmoid     : {brier_sig:.4f}\")\n",
    "\n",
    "\n",
    "# üì¶ Sauvegarde du meilleur mod√®le calibr√©\n",
    "joblib.dump(best_global_model, \"../models/tennis_win_predictor.pkl\")\n",
    "\n",
    "# üíæ Sauvegarde des performances\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"ROC AUC\", ascending=False)\n",
    "results_df.to_csv(\"../models/evaluation_models.csv\", index=False)\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur mod√®le global : {best_global_model_name}\")\n",
    "print(f\"üìà Score global obtenu : {best_global_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# üéØ Utiliser le bon X_test / y_test selon le mod√®le choisi\n",
    "X_test_eval = X_test_bt if best_global_model_name == \"BradleyTerry_LogReg\" else X_test\n",
    "y_test_eval = y_test_bt if best_global_model_name == \"BradleyTerry_LogReg\" else y_test\n",
    "\n",
    "y_proba_final = best_global_model.predict_proba(X_test_eval)[:, 1]\n",
    "\n",
    "# ‚ö° D√©finir les \"matchs √† haute confiance\"\n",
    "high_conf_threshold = 0.65\n",
    "mask_high_conf = (y_proba_final > high_conf_threshold) | (y_proba_final < (1 - high_conf_threshold))\n",
    "\n",
    "# üß™ Extraire les lignes concern√©es\n",
    "df_base_test = df.loc[X_test_eval.index].copy()\n",
    "df_high_conf = df_base_test.loc[mask_high_conf].copy()\n",
    "df_high_conf[\"Proba_Player1\"] = y_proba_final[mask_high_conf]\n",
    "df_high_conf[\"Predicted_Winner\"] = (df_high_conf[\"Proba_Player1\"] >= 0.5).astype(int)\n",
    "df_high_conf[\"Is_Correct\"] = (df_high_conf[\"Predicted_Winner\"] == y_test_eval.iloc[mask_high_conf].values).astype(int)\n",
    "\n",
    "# üìä Afficher les r√©sultats\n",
    "print(f\"{len(df_high_conf)} matchs √† haute confiance (proba > {high_conf_threshold})\")\n",
    "print(f\"Taux de r√©ussite sur ces matchs : {df_high_conf['Is_Correct'].mean():.2%}\")\n",
    "\n",
    "# Facultatif : afficher un √©chantillon\n",
    "df_high_conf[[\"Player_1\", \"Player_2\", \"Proba_Player1\", \"Predicted_Winner\", \"Is_Correct\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d4799",
   "metadata": {},
   "source": [
    "#### Comparaison visuelle de la fiabilit√© des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# D√©terminer les bons X_test / y_test selon le mod√®le choisi\n",
    "X_test_eval = X_test_bt if best_global_model_name == \"BradleyTerry_LogReg\" else X_test\n",
    "y_test_eval = y_test_bt if best_global_model_name == \"BradleyTerry_LogReg\" else y_test\n",
    "\n",
    "# Probabilit√©s pr√©dictes finales\n",
    "y_proba_final = best_global_model.predict_proba(X_test_eval)[:, 1]\n",
    "\n",
    "# Brier Score final\n",
    "brier_final = brier_score_loss(y_test_eval, y_proba_final)\n",
    "\n",
    "# Courbe de calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test_eval, y_proba_final, n_bins=10)\n",
    "\n",
    "# üîç FIGURE COMBIN√âE\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 1. Courbe de calibration\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Calibr√©')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', label='Perfect calibration')\n",
    "plt.xlabel(\"Probabilit√© pr√©dite\")\n",
    "plt.ylabel(\"Proportion r√©elle de victoire\")\n",
    "plt.title(f\"üìê Calibration - {best_global_model_name}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Histogramme des proba\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_proba_final, bins=20, kde=False, color='steelblue')\n",
    "plt.title(\"üìä Distribution des probabilit√©s pr√©dites\")\n",
    "plt.xlabel(\"Probabilit√© victoire Player 1\")\n",
    "plt.ylabel(\"Nombre de matchs\")\n",
    "\n",
    "plt.suptitle(f\"üîç Fiabilit√© du mod√®le : {best_global_model_name}\\nBrier Score = {brier_final:.4f}\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bb5ff",
   "metadata": {},
   "source": [
    "#### V√©rifier les pr√©dictions peu tranch√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Refaire une pr√©diction sur X_test pour visualiser les probabilit√©s\n",
    "y_proba_test = best_global_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "plt.hist(y_proba_test, bins=20, edgecolor='k')\n",
    "plt.title(\"Distribution des probabilit√©s de victoire (Player_1)\")\n",
    "plt.xlabel(\"Probabilit√© pr√©dite\")\n",
    "plt.ylabel(\"Nombre de matchs\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0167e",
   "metadata": {},
   "source": [
    "#### Visulaiser les pr√©dictions incertaines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_proba_test, bins=20, edgecolor='k')\n",
    "plt.axvline(0.5, color='red', linestyle='--')  # seuil\n",
    "plt.title(\"Distribution des probas (Player 1)\")\n",
    "plt.xlabel(\"Proba de victoire Player 1\")\n",
    "plt.ylabel(\"Nombre de matchs\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51940326",
   "metadata": {},
   "source": [
    "#### Histogramme des probabilit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme des probabilit√©s\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_proba_final, bins=20, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Zones de haute confiance\n",
    "plt.axvline(high_conf_threshold, color='green', linestyle='--', label=f'Haute confiance > {high_conf_threshold}')\n",
    "plt.axvline(1 - high_conf_threshold, color='red', linestyle='--', label=f'Haute confiance < {1 - high_conf_threshold}')\n",
    "\n",
    "plt.title(\"Distribution des probabilit√©s pr√©dites (Player 1 gagnant)\")\n",
    "plt.xlabel(\"Probabilit√© pr√©dite\")\n",
    "plt.ylabel(\"Nombre de matchs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d9291",
   "metadata": {},
   "source": [
    "#### Diagramme de fiabilit√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeaa924",
   "metadata": {},
   "source": [
    "### Entraine un mod√®le de r√©gression pour le nombre de jeux du match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Donn√©es\n",
    "X_gc = df[all_features]\n",
    "y_gc = df[\"Games_Class\"]\n",
    "\n",
    "Xgc_train, Xgc_test, ygc_train, ygc_test = train_test_split(X_gc, y_gc, test_size=0.2, random_state=42, stratify=y_gc)\n",
    "\n",
    "# üì¶ Mod√®les √† tester\n",
    "models_gc = {\n",
    "    \"RandomForest\": (\n",
    "        Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"clf\", RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        {\"clf__n_estimators\": [50, 150], \"clf__max_depth\": [5, 10, None]}\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"clf\", GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        {\"clf__n_estimators\": [50, 150], \"clf__learning_rate\": [0.05, 0.1]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# üîÅ Fonction de score global\n",
    "def compute_classification_score(acc, bal_acc, f1, auc, prec):\n",
    "    return (\n",
    "        0.3 * acc +\n",
    "        0.2 * bal_acc +\n",
    "        0.2 * f1 +\n",
    "        0.2 * auc +\n",
    "        0.1 * prec\n",
    "    )\n",
    "\n",
    "# üîç √âvaluation\n",
    "results_gc = []\n",
    "best_model_gc = None\n",
    "best_score_gc = float('-inf')\n",
    "\n",
    "for name, (model, params) in models_gc.items():\n",
    "    print(f\"\\nüîç Entra√Ænement de {name}...\")\n",
    "\n",
    "    grid = GridSearchCV(model, params, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    grid.fit(Xgc_train, ygc_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(Xgc_test)\n",
    "\n",
    "    # Probabilit√©s pour ROC AUC\n",
    "    try:\n",
    "        y_proba = best_model.predict_proba(Xgc_test)\n",
    "    except:\n",
    "        y_proba = None\n",
    "\n",
    "    # M√©triques\n",
    "    acc = accuracy_score(ygc_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(ygc_test, y_pred)\n",
    "    f1 = f1_score(ygc_test, y_pred, average=\"macro\")\n",
    "    prec = precision_score(ygc_test, y_pred, average=\"macro\")\n",
    "    rec = recall_score(ygc_test, y_pred, average=\"macro\")\n",
    "\n",
    "    if y_proba is not None:\n",
    "        classes = best_model.named_steps[\"clf\"].classes_\n",
    "        ygc_test_bin = label_binarize(ygc_test, classes=classes)\n",
    "        auc = roc_auc_score(ygc_test_bin, y_proba, average='macro', multi_class='ovr')\n",
    "    else:\n",
    "        auc = 0  # Pas dispo pour ce mod√®le\n",
    "\n",
    "    score = compute_classification_score(acc, bal_acc, f1, auc, prec)\n",
    "\n",
    "    print(f\"‚úÖ Best Params: {grid.best_params_}\")\n",
    "    print(f\"üìä Accuracy: {acc:.3f}\")\n",
    "    print(f\"üìà Balanced Accuracy: {bal_acc:.3f}\")\n",
    "    print(f\"üéØ F1 (macro): {f1:.3f}\")\n",
    "    print(f\"üìâ ROC AUC: {auc:.3f}\")\n",
    "    print(f\"üèÜ Score global: {score:.4f}\")\n",
    "\n",
    "    results_gc.append({\n",
    "        \"Mod√®le\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Balanced Accuracy\": bal_acc,\n",
    "        \"F1\": f1,\n",
    "        \"ROC AUC\": auc,\n",
    "        \"Score Global\": score,\n",
    "        \"Best Params\": grid.best_params_\n",
    "    })\n",
    "\n",
    "    if score > best_score_gc:\n",
    "        best_score_gc = score\n",
    "        best_model_gc = best_model\n",
    "        best_global_model_name = name\n",
    "\n",
    "# üíæ Sauvegarde du meilleur mod√®le\n",
    "joblib.dump(best_model_gc, f\"../models/tennis_total_games_predictor.pkl\")\n",
    "\n",
    "# üìã R√©sum√©\n",
    "results_df = pd.DataFrame(results_gc).sort_values(by=\"Score Global\", ascending=False)\n",
    "print(\"\\nüìä R√©sum√© des performances (classe de jeux) :\")\n",
    "print(results_df[[\"Mod√®le\", \"Accuracy\", \"Balanced Accuracy\", \"F1\", \"ROC AUC\", \"Score Global\"]])\n",
    "\n",
    "# üëâ Affichage du meilleur mod√®le selon ton score global\n",
    "print(f\"\\nüèÜ Meilleur mod√®le global : {best_global_model_name}\")\n",
    "print(f\"üìà Score global obtenu : {best_score_gc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd4717",
   "metadata": {},
   "source": [
    "### üîÆ Fonctions de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2666faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10. üîÆ Pr√©diction sur un match √† partir d‚Äôun dictionnaire\n",
    "def predict_match_proba(input_dict, model_path=\"../models/tennis_win_predictor.pkl\"):\n",
    "    model = joblib.load(model_path)\n",
    "    enriched_input = enrich_features(input_dict)  # ‚ö†Ô∏è Doit g√©n√©rer TOUTES les features\n",
    "    X_input = pd.DataFrame([enriched_input])\n",
    "    prob = model.predict_proba(X_input)[0]\n",
    "    return {\n",
    "        \"Probabilit√© victoire Player_1\": round(prob[1], 4),\n",
    "        \"Probabilit√© victoire Player_2\": round(prob[0], 4)\n",
    "    }\n",
    "\n",
    "\n",
    "### 10. üîÆ Pr√©diction du nombre de jeux\n",
    "def predict_total_games(input_dict, model_path=\"../models/tennis_total_games_predictor.pkl\"):\n",
    "    model = joblib.load(model_path)\n",
    "    X_input = pd.DataFrame([input_dict])\n",
    "    prediction = model.predict(X_input)[0]\n",
    "    return prediction\n",
    "\n",
    "def enrich_features(input_dict, h2h_dict=None, recent_form_dict=None):\n",
    "    enriched = input_dict.copy()\n",
    "    \n",
    "    # Calculs classiques\n",
    "    enriched[\"Rank_Diff\"] = enriched[\"Rank_1\"] - enriched[\"Rank_2\"]\n",
    "    enriched[\"Pts_Diff\"] = enriched[\"Pts_1\"] - enriched[\"Pts_2\"]\n",
    "    enriched[\"Odds_Diff\"] = abs(enriched[\"Odd_1\"] - enriched[\"Odd_2\"])\n",
    "    enriched[\"Odds_Ratio\"] = enriched[\"Odd_1\"] / enriched[\"Odd_2\"] if enriched[\"Odd_2\"] > 0 else 0\n",
    "    enriched[\"Avg_Rank\"] = (enriched[\"Rank_1\"] + enriched[\"Rank_2\"]) / 2\n",
    "    enriched[\"Book_Fav\"] = int(enriched[\"Odd_1\"] < enriched[\"Odd_2\"])\n",
    "\n",
    "    # Round encod√©\n",
    "    round_mapping = {\n",
    "        \"Final\": 7, \"Semifinal\": 6, \"Quarterfinal\": 5,\n",
    "        \"4th Round\": 4, \"3rd Round\": 3, \"2nd Round\": 2, \"1st Round\": 1\n",
    "    }\n",
    "    enriched[\"Round_Ordinal\"] = round_mapping.get(enriched.get(\"Round\", \"\"), 0)\n",
    "\n",
    "    # H2H\n",
    "    if h2h_dict:\n",
    "        key = tuple(sorted([enriched[\"Player_1\"], enriched[\"Player_2\"]]))\n",
    "        h2h = h2h_dict.get(key, [0, 0])\n",
    "        if enriched[\"Player_1\"] > enriched[\"Player_2\"]:\n",
    "            h2h = h2h[::-1]\n",
    "        enriched[\"H2H_P1\"], enriched[\"H2H_P2\"] = h2h\n",
    "        enriched[\"H2H_Diff\"] = h2h[0] - h2h[1]\n",
    "    else:\n",
    "        enriched[\"H2H_P1\"] = enriched[\"H2H_P2\"] = enriched[\"H2H_Diff\"] = 0\n",
    "\n",
    "    # Forme r√©cente\n",
    "    if recent_form_dict:\n",
    "        enriched[\"Wins_Last5_P1\"] = recent_form_dict.get(enriched[\"Player_1\"], 0)\n",
    "        enriched[\"Wins_Last5_P2\"] = recent_form_dict.get(enriched[\"Player_2\"], 0)\n",
    "        enriched[\"Form_Diff\"] = enriched[\"Wins_Last5_P1\"] - enriched[\"Wins_Last5_P2\"]\n",
    "    else:\n",
    "        enriched[\"Wins_Last5_P1\"] = enriched[\"Wins_Last5_P2\"] = enriched[\"Form_Diff\"] = 0\n",
    "\n",
    "    return enriched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1a413",
   "metadata": {},
   "source": [
    "## Pr√©diction avec mod√®le optimis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ea614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "exemple_match = {\n",
    "    \"Player_1\": \"Safiullin R.\",\n",
    "    \"Player_2\": \"Mensik J.\",\n",
    "    \"Rank_1\": 71,\n",
    "    \"Rank_2\": 54,\n",
    "    \"Pts_1\": 851,\n",
    "    \"Pts_2\": 1042,\n",
    "    \"Odd_1\": 2.57,\n",
    "    \"Odd_2\": 1.49,\n",
    "    \"Surface\": \"Hard\",\n",
    "    \"Round\": \"3rd Round\",\n",
    "    \"Best of\": 3,\n",
    "    \"Court\": \"Outdoor\"\n",
    "}\n",
    "\n",
    "\n",
    "h2h_dict = joblib.load(\"../models/h2h_dict.pkl\")\n",
    "recent_form_dict = joblib.load(\"../models/recent_form_dict.pkl\")\n",
    "\n",
    "\n",
    "# üõ†Ô∏è Ajout des features manquantes\n",
    "enriched_input = enrich_features(exemple_match, h2h_dict, recent_form_dict)\n",
    "\n",
    "# üî¢ Pr√©diction du vainqueur\n",
    "probas = predict_match_proba(enriched_input, model_path=\"../models/tennis_win_predictor.pkl\")\n",
    "\n",
    "# üî¢ Pr√©diction total jeux\n",
    "predicted_class = predict_total_games(enriched_input, model_path=\"../models/tennis_total_games_predictor.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüéØ Pr√©diction compl√®te :\")\n",
    "for joueur, proba in probas.items():\n",
    "    print(f\"{joueur} : {proba:.2%}\")\n",
    "print(f\"üìä Intervalle pr√©dit pour le nombre de jeux : {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
